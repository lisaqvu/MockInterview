{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pylab import *\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "\n",
    "df = pd.read_csv(\"inter2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 2], [3, 3, 3, 3, 2, 3], [3, 1, 3, 3, 3, 3], [3, 3, 3, 3, 1, 1], [3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3], [3, 3, 3, 2, 1, 1], [3, 3, 3, 2, 1, 1], [3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3], [1, 1, 1, 1, 1, 1], [2, 2, 3, 3, 1, 1], [3, 2, 3, 3, 1, 1], [3, 3, 3, 3, 1, 1], [3, 3, 3, 3, 3, 3], [3, 3, 3, 1, 2, 2], [3, 3, 3, 1, 1, 1], [3, 3, 3, 1, 1, 1], [3, 3, 3, 1, 1, 1], [2, 3, 3, 3, 3, 2], [3, 3, 3, 1, 1, 1], [3, 3, 3, 1, 1, 1], [3, 3, 3, 3, 1, 1], [2, 3, 3, 1, 2, 1], [3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3]]\n"
     ]
    }
   ],
   "source": [
    "## For extracting the features from the csv file\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "f = open(\"inter2.csv\", \"rb\")\n",
    "reader = csv.reader(f)\n",
    "target=[]\n",
    "k=0\n",
    "for row in reader:\n",
    "    if(k>0):\n",
    "        temp=[]\n",
    "        for i in row[1:7]:\n",
    "            temp.append(int(i))\n",
    "        target.append(temp)\n",
    "    k+=1    \n",
    "\n",
    "print target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Forr opening that file\n",
    "\n",
    "import pickle\n",
    "f=open('text_answer.txt','r')\n",
    "new_text=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## For storing variables in a data file as text format\n",
    "\n",
    "import pickle\n",
    "f=open('text_answer.txt','w')\n",
    "pickle.dump(text,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text[1]=new_text[1].replace(\"  \",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello.', 'And']"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Hello. And\".split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('So', 'RB'), ('challenge', 'JJ'), (\"I've\", 'NNP'), ('overcame', 'NN'), ('was', 'VBD'), ('having', 'VBG'), ('a', 'DT'), ('primarily', 'RB'), ('business', 'NN'), ('backgrounds', 'NNS'), ('and', 'CC'), ('coming', 'VBG'), ('into', 'IN'), ('completely', 'RB'), ('technology-based', 'JJ'), ('company,', 'NN'), ('kind', 'NN'), ('of', 'IN'), ('like', 'IN'), ('finding', 'VBG'), ('a', 'DT'), ('role', 'NN'), ('for', 'IN'), ('myself', 'PRP'), ('and', 'CC'), ('being', 'VBG'), ('able', 'JJ'), ('to..', 'NN'), ('It', 'PRP'), ('was', 'VBD'), ('kind', 'NN'), ('of', 'IN'), ('hard', 'JJ'), ('because', 'IN'), ('I', 'PRP'), ('had', 'VBD'), ('to', 'TO'), ('kind', 'NN'), ('of', 'IN'), ('learn', 'NN'), ('all', 'PDT'), ('those', 'DT'), ('new', 'JJ'), ('things', 'NNS'), ('and', 'CC'), ('not', 'RB'), ('even', 'RB'), ('just', 'RB'), ('ffrom', 'VB'), ('being', 'VBG'), ('able', 'JJ'), ('to', 'TO'), ('do', 'VB'), ('them', 'PRP'), ('but', 'CC'), ('being', 'VBG'), ('able', 'JJ'), ('to', 'TO'), ('understand', 'VB'), ('how', 'WRB'), ('they', 'PRP'), ('workand', 'VBP'), ('how', 'WRB'), ('that', 'DT'), ('applies', 'VBZ'), ('to', 'TO'), ('the', 'DT'), ('business.', 'NN'), ('And', 'CC'), ('then', 'RB'), ('also', 'RB'), ('now', 'RB'), (\"I'm\", 'NNP'), ('doing', 'VBG'), ('development', 'NN'), ('work', 'NN'), ('so', 'RB'), ('that', 'RB'), ('has', 'VBZ'), ('its', 'PRP$'), ('challenges', 'NNS'), ('in', 'IN'), ('itself', 'PRP'), ('but', 'CC'), ('like', 'IN'), ('Jentry', 'NNP'), ('said', 'VBD'), (\"we're\", 'NN'), ('on', 'IN'), ('the', 'DT'), ('same', 'JJ'), ('team,', 'NN'), ('we', 'PRP'), ('do', 'VBP'), ('have', 'VB'), ('a', 'DT'), ('team', 'NN'), ('member', 'NN'), ('who', 'WP'), ('is', 'VBZ'), ('very', 'RB'), ('very', 'RB'), ('good', 'JJ'), ('at', 'IN'), ('explaining', 'VBG'), ('things,', 'JJ'), (\"he's\", 'JJ'), ('very', 'RB'), ('patient', 'JJ'), ('and', 'CC'), ('so', 'RB'), ('when', 'WRB'), ('I', 'PRP'), ('if', 'IN'), (\"I'm\", 'NNP'), ('trying', 'VBG'), ('my', 'PRP$'), ('hardest', 'NN'), ('with', 'IN'), ('one', 'CD'), ('thing,', 'NN'), ('trying', 'VBG'), ('multiple', 'JJ'), ('different', 'JJ'), ('ways', 'NNS'), ('and', 'CC'), (\"can't\", 'JJ'), ('figure', 'NN'), ('it', 'PRP'), ('out', 'RP'), ('I', 'PRP'), ('go', 'VBP'), ('to', 'TO'), ('him', 'PRP'), ('and', 'CC'), ('say', 'VB'), ('hey,', 'NN'), ('like,', 'NN'), ('this', 'DT'), ('is', 'VBZ'), ('what', 'WP'), (\"I've\", 'NNP'), ('worked', 'VBD'), ('worked', 'JJ'), ('with,', 'NN'), ('this', 'DT'), ('is', 'VBZ'), ('what', 'WP'), (\"I've\", 'NNP'), ('done.', 'NN'), ('Help', 'NNP'), ('me', 'PRP'), ('out.', 'JJ'), ('And', 'CC'), (\"that's\", 'JJ'), ('been', 'VBN'), ('really', 'RB'), ('helpful', 'JJ'), ('so', 'RB'), ('just', 'RB'), ('kinda', 'VB'), ('like', 'IN'), ('knowing', 'VBG'), ('when', 'WRB'), ('to', 'TO'), ('ask,', 'VB'), ('trying', 'VBG'), ('first', 'JJ'), ('and', 'CC'), ('knowing', 'VBG'), (\"that's\", 'JJ'), (\"it's\", 'NN'), ('okay', 'NN'), ('to', 'TO'), ('ask', 'VB'), ('for', 'IN'), ('help', 'NN'), ('instead', 'RB'), ('of', 'IN'), ('wasting', 'VBG'), ('a', 'DT'), ('lot', 'NN'), ('of', 'IN'), ('time', 'NN'), ('just', 'RB'), ('like', 'IN'), ('banging', 'VBG'), ('your', 'PRP$'), ('head', 'NN'), ('against', 'IN'), ('the', 'DT'), ('wall', 'NN'), ('yeah.', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "res=nltk.pos_tag(new_text[1].split(\" \"))\n",
    "print res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RB', 'JJ', 'NNP', 'NN', 'VBD', 'VBG', 'DT', 'RB', 'NN', 'NNS', 'CC', 'VBG', 'IN', 'RB', 'JJ', 'NN', 'NN', 'IN', 'IN', 'VBG', 'DT', 'NN', 'IN', 'PRP', 'CC', 'VBG', 'JJ', 'NN', 'PRP', 'VBD', 'NN', 'IN', 'JJ', 'IN', 'PRP', 'VBD', 'TO', 'NN', 'IN', 'NN', 'PDT', 'DT', 'JJ', 'NNS', 'CC', 'RB', 'RB', 'RB', 'VB', 'VBG', 'JJ', 'TO', 'VB', 'PRP', 'CC', 'VBG', 'JJ', 'TO', 'VB', 'WRB', 'PRP', 'VBP', 'WRB', 'DT', 'VBZ', 'TO', 'DT', 'NN', 'CC', 'RB', 'RB', 'RB', 'NNP', 'VBG', 'NN', 'NN', 'RB', 'RB', 'VBZ', 'PRP$', 'NNS', 'IN', 'PRP', 'CC', 'IN', 'NNP', 'VBD', 'NN', 'IN', 'DT', 'JJ', 'NN', 'PRP', 'VBP', 'VB', 'DT', 'NN', 'NN', 'WP', 'VBZ', 'RB', 'RB', 'JJ', 'IN', 'VBG', 'JJ', 'JJ', 'RB', 'JJ', 'CC', 'RB', 'WRB', 'PRP', 'IN', 'NNP', 'VBG', 'PRP$', 'NN', 'IN', 'CD', 'NN', 'VBG', 'JJ', 'JJ', 'NNS', 'CC', 'JJ', 'NN', 'PRP', 'RP', 'PRP', 'VBP', 'TO', 'PRP', 'CC', 'VB', 'NN', 'NN', 'DT', 'VBZ', 'WP', 'NNP', 'VBD', 'JJ', 'NN', 'DT', 'VBZ', 'WP', 'NNP', 'NN', 'NNP', 'PRP', 'JJ', 'CC', 'JJ', 'VBN', 'RB', 'JJ', 'RB', 'RB', 'VB', 'IN', 'VBG', 'WRB', 'TO', 'VB', 'VBG', 'JJ', 'CC', 'VBG', 'JJ', 'NN', 'NN', 'TO', 'VB', 'IN', 'NN', 'RB', 'IN', 'VBG', 'DT', 'NN', 'IN', 'NN', 'RB', 'IN', 'VBG', 'PRP$', 'NN', 'IN', 'DT', 'NN', 'NN']\n"
     ]
    }
   ],
   "source": [
    "tags_list=[0]*len(res)\n",
    "for i in range(len(res)):\n",
    "    tags_list[i]=res[i][1]\n",
    "print tags_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CC', 'CD', 'DT', 'IN', 'JJ', 'NN', 'NNP', 'NNS', 'PRP', 'PRP$', 'RB', 'RP', 'TO', 'VB', 'VBD', 'VBG', 'VBN', 'VBZ', 'WDT', 'WP', 'WRB']\n"
     ]
    }
   ],
   "source": [
    "tags_list=list(sorted(set(tags_list)))\n",
    "print tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=['EX','FW','JJR','JJS','LS','MD','NNPS','PDT','POS','RBR','RBS','SYM','UH','VBP','WP$','WDT',',']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n"
     ]
    }
   ],
   "source": [
    "tags_final=sorted([',','CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']) \n",
    "print tags_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "inputs = []\n",
    "\n",
    "for text in new_text:\n",
    "    \n",
    "    #print text\n",
    "    text = text.strip()\n",
    "    text=text.replace(\"  \",\" \")\n",
    "    \n",
    "    #print(len(nltk.pos_tag(text.split(\" \"))))\n",
    "    res=nltk.pos_tag(text.split(\" \"))\n",
    "    \n",
    "#     print res\n",
    "    p=len(res)\n",
    "    #print p\n",
    "    tags_list=[0]*p\n",
    "    for j in range(p):\n",
    "        tags_list[j]=res[j][1]\n",
    "\n",
    "    tags =collections.OrderedDict()\n",
    "    for k in range(len(tags_final)):\n",
    "        tags[tags_final[k]] = 0\n",
    "\n",
    "\n",
    "    for l in range(len(tags_list)):\n",
    "        tags[tags_list[l]]+=1\n",
    "        \n",
    "    inputs.append(tags.values())\n",
    "    #print(tags.values())\n",
    "    \n",
    "print len(inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "f=open('input_features.txt','w')\n",
    "pickle.dump(inputs,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs[27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = df[\"Transcription\"]\n",
    "print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#quantify answer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# For beginning, transform train['FullDescription'] to lowercase using text.lower()\n",
    "df['Transcription'].str.lower()\n",
    "\n",
    "# Then replace everything except the letters and numbers in the spaces.\n",
    "# it will facilitate the further division of the text into words.\n",
    "df['Transcription'].replace('[^a-zA-Z0-9]', ' ', regex = True)\n",
    "\n",
    "# Convert a collection of raw documents to a matrix of TF-IDF features with TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=5)\n",
    "X_tfidf = vectorizer.fit_transform(df['Transcription'])\n",
    "\n",
    "print X_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split to test data\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, target, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.98497962  2.97800872  3.          3.53928708  2.13299474  2.61462144]\n",
      " [ 2.82914253  3.05647585  3.          3.54088512  2.59957144  3.11639111]\n",
      " [ 2.90782314  3.13987157  3.          0.81053293  1.59793785  1.75499517]\n",
      " [ 2.01002703  3.09867721  3.         -0.63784978  3.38037693  3.24340966]\n",
      " [ 2.88513678  5.39147095  3.          2.68093078  3.76879032  3.77087388]\n",
      " [ 3.27380266  3.23912789  3.          2.69611816  2.00022132  1.79510593]\n",
      " [ 2.85171652  2.81565343  3.          2.38140763  1.05775944  1.12826948]]\n",
      "Error is  0.869252914645648\n"
     ]
    }
   ],
   "source": [
    "# fit a model\n",
    "lm =  linear_model.LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "predictions = lm.predict(X_test)\n",
    "print(predictions)\n",
    "\n",
    "# error\n",
    "error=np.mean(abs(predictions-y_test))\n",
    "print\"Error is \",error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm.predict(feature_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predictions)):\n",
    "    for j in range(len(predictions[i])):\n",
    "        predictions[i][j]=round(predictions[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-300-e8e35b7a2fb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "import math\n",
    "math.ceil(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error is  0.8809523809523809\n"
     ]
    }
   ],
   "source": [
    "# error\n",
    "error=np.mean(abs(predictions-y_test))\n",
    "print\"Error is \",error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.,  3.,  3.,  4.,  2.,  3.],\n",
       "       [ 3.,  3.,  3.,  4.,  3.,  3.],\n",
       "       [ 3.,  3.,  3.,  1.,  2.,  2.],\n",
       "       [ 2.,  3.,  3., -1.,  3.,  3.],\n",
       "       [ 3.,  5.,  3.,  3.,  4.,  4.],\n",
       "       [ 3.,  3.,  3.,  3.,  2.,  2.],\n",
       "       [ 3.,  3.,  3.,  2.,  1.,  1.]])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 3, 3, 2, 1, 1],\n",
       " [2, 3, 3, 1, 2, 1],\n",
       " [3, 3, 3, 2, 1, 1],\n",
       " [2, 3, 3, 3, 3, 2],\n",
       " [3, 3, 3, 3, 3, 3],\n",
       " [1, 1, 1, 1, 1, 1],\n",
       " [3, 3, 3, 1, 2, 2]]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 2. 1. 2.]\n",
      " [1. 0. 0. 3. 1. 2.]\n",
      " [0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 4. 0. 1.]\n",
      " [0. 2. 0. 0. 1. 1.]\n",
      " [2. 2. 2. 2. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "t=abs(predictions-y_test)\n",
    "print t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.0"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=37.0/(6*7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8809523809523809"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
